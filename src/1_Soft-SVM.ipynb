{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft SVM Implementation for Determining Credit Card Fraud.\n",
    "\n",
    "Dataset: [Kaggle-Credit Card Fraud Dataset](https://paperswithcode.com/dataset/kaggle-credit-card-fraud-dataset)\n",
    "\n",
    "_Data has already undergone PCA and preprocessing to anonymize the data._\n",
    "\n",
    "The original classes for the data are:\n",
    "\n",
    "* Fraudulent Transaction: _+1_\n",
    "\n",
    "* Non-fraudulent Transaction: _0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and modify the class values for SVM.\n",
    "\n",
    "The modified classes for the data are:\n",
    "\n",
    "* Fraudulent Transaction: _+1_\n",
    "\n",
    "* Non-fraudulent Transaction: _-1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data: pd.DataFrame = pd.read_csv(\"../creditcard.csv\")\n",
    "data[\"Class\"] = np.where(data[\"Class\"] <= 0, -1, 1)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62     -1  \n",
       "1  0.125895 -0.008983  0.014724    2.69     -1  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66     -1  \n",
       "3 -0.221929  0.062723  0.061458  123.50     -1  \n",
       "4  0.502292  0.219422  0.215153   69.99     -1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training/testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    data, test_size=0.2, stratify=data[\"Class\"], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Soft-SVM\n",
    "\n",
    "### Soft-SVM Objective\n",
    "\n",
    "$$\\min_{w,b} \\frac{1}{2} ||w||^2 + C \\sum_i \\xi_i$$\n",
    "\n",
    "$\\text{s.t. } y_i(w^T x_i + b) \\geq 1 - \\xi_i$\n",
    "\n",
    "$\\xi_i \\geq 0$\n",
    "\n",
    "$\\text{for } i \\in \\{1, 2, ..., N\\}$\n",
    "\n",
    "### Slack variable $\\xi$\n",
    "\n",
    "The misclassified points\n",
    "\n",
    "$$\\xi_i = \\max(1 - y_i(w^T x_i + b), 0)$$\n",
    "\n",
    "* If $\\xi_i = 0$, correctly classified point outside of the margin.\n",
    "\n",
    "* If $0 < \\xi_i < 1$, correctly classified point within the margin.\n",
    "\n",
    "* If $\\xi_i \\geq 1$, misclassified point.\n",
    "\n",
    "### Hinge-Loss Function\n",
    "\n",
    "- Counting the distance of the misclassified points from the margin.\n",
    "\n",
    "$$\\min_{w,b} \\frac{1}{2} ||w||^2 + C \\sum_i \\xi_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(\n",
    "    X: np.ndarray, Y: np.ndarray, weights: np.ndarray, bias: float, C: float\n",
    ") -> float:\n",
    "    \"\"\"Computes the loss for the current weights and bias.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The features.\n",
    "        Y (np.ndarray): The labels (either -1 or 1).\n",
    "        weights (np.ndarray): The current weights of the model.\n",
    "        bias (float): The current bias of the model.\n",
    "        C (float): Soft-SVM hyperparameter for adjusting margin size.\n",
    "\n",
    "    Returns:\n",
    "        float: The regularized hinge-loss value for the current model.\n",
    "    \"\"\"\n",
    "    # Calculate the slack variable\n",
    "    hinge = np.maximum(1 - Y * (np.dot(X, weights) + bias), 0)\n",
    "\n",
    "    # Return the hinge-loss\n",
    "    return 0.5 * np.linalg.norm(weights) ** 2 + C * np.sum(hinge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Weights and Bias\n",
    "\n",
    "1. Initialize weights and bias to 0\n",
    "\n",
    "2. Determine the margin, or the distance that each sample is from the margin.\n",
    "    - $m_i = 1- y_i (w^T x_i + b)$\n",
    "\n",
    "3. Determine the weight gradient:\n",
    "    - $\\nabla_w = w - \\frac{C}{n} \\sum_{i, \\text{where } m_i > 0}^n y_i x_i$\n",
    "    - Only use points that are either in the margin or misclassified.\n",
    "\n",
    "4. Determine the bias gradient:\n",
    "    - $\\nabla_b = - \\frac{C}{n} \\sum_{i, \\text{where } m_i > 0}^n y_i$\n",
    "    - Only use points that are either in the margin or misclassified.\n",
    "\n",
    "5. Update the weights and bias:\n",
    "    - $w \\leftarrow w - \\eta \\nabla_w$\n",
    "    - $b \\leftarrow b - \\eta \\nabla_b$\n",
    "\n",
    "- Repeat 2-5 per epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    learning_rate: float,\n",
    "    C: float,\n",
    "    epochs: int,\n",
    "    class_weights: dict[int, float],\n",
    "    decay: float,\n",
    "    log: bool = True,\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Fits weights and bias for the input data and labels.\n",
    "\n",
    "    A linear SVM model.\n",
    "\n",
    "    Weights and bias are initialized to zero.\n",
    "\n",
    "    The program does the following for each epoch:\n",
    "    - Computes the margin given the current weights and biases for all samples.\n",
    "    - Updates the weights using misclassified points which are determined using the margin.\n",
    "    - Updates the bias using the misclassified points which are determined using the margin.\n",
    "    - Display the current hinge loss every 50 epochs.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The features.\n",
    "        Y (np.ndarray): The labels.\n",
    "        learning_rate (float): The learning rate.\n",
    "        C (float): Soft-SVM hyperparameter for adjusting margin size.\n",
    "        epochs (int): The number of epochs the model will run.\n",
    "        class_weights (dict[int, float]): Extra weight provided to each class. Keys should be -1 and 1.\n",
    "        decay (float): The rate that the learning rate decays.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, float]: The final weights and bias.\n",
    "    \"\"\"\n",
    "    assert set(class_weights.keys()) == {-1, 1}\n",
    "\n",
    "    num_samples, num_features = X.shape\n",
    "\n",
    "    # Initialize weights as small, random values.\n",
    "    weights: np.ndarray = np.random.randn(num_features) * 0.025\n",
    "\n",
    "    # Initialize weights as zeros.\n",
    "    # weights: np.ndarray = np.zeros(num_features)\n",
    "\n",
    "    # Initialize bias as the log-ratio of class weights.\n",
    "    bias: float = np.log(class_weights[1] / class_weights[-1])\n",
    "\n",
    "    # Initialize bias as zero.\n",
    "    # bias: float = 0.0\n",
    "\n",
    "    time_0 = time.time()\n",
    "\n",
    "    base_learning_rate: float = learning_rate\n",
    "    losses: list[float] = list()\n",
    "    for epoch in range(epochs):\n",
    "        # Determine SVM Margins\n",
    "        margins: np.ndarray = 1 - Y * (np.dot(X, weights) + bias)\n",
    "\n",
    "        C_weights: np.ndarray = np.where(\n",
    "            Y == 1, C * class_weights[1], C * class_weights[-1]\n",
    "        )\n",
    "\n",
    "        # Determine Gradients - updated using points that are in the margin or misclassified.\n",
    "        grad_weights = (\n",
    "            weights - np.dot((C_weights * (margins > 0) * Y), X) / num_samples\n",
    "        )\n",
    "        grad_bias = np.sum(-C_weights * (margins > 0) * Y) / num_samples\n",
    "\n",
    "        # Clip Gradients\n",
    "        grad_weights = np.clip(grad_weights, -1.0, 1.0)\n",
    "        grad_bias = np.clip(grad_bias, -1.0, 1.0)\n",
    "\n",
    "        # Update Weights and Biases\n",
    "        weights -= learning_rate * grad_weights\n",
    "        bias -= learning_rate * grad_bias\n",
    "\n",
    "        # Decay Learning Rate\n",
    "        # learning_rate = 0.001 * (1.0 / (1.0 + 0.001 * epoch))\n",
    "        learning_rate = base_learning_rate / (1.0 + decay * epoch)\n",
    "\n",
    "        # Print the loss at every 50th epoch\n",
    "        if log and (epoch % 50 == 0 or epoch == epochs - 1):\n",
    "            # Timer Stuff\n",
    "            time_1 = time.time()\n",
    "            time_diff = time_1 - time_0\n",
    "            time_0 = time_1\n",
    "\n",
    "            losses.append(hinge_loss(X, Y, weights, bias, C))\n",
    "            loss_diff: float = (\n",
    "                losses[-1] - losses[-2] if len(losses) >= 2 else 0.0\n",
    "            )\n",
    "            if log:\n",
    "                print(\n",
    "                    f\"Epoch {epoch:<5} Loss = {losses[-1]:<10.2e}\"\n",
    "                    + (\"\\033[92m\" if loss_diff < 0 else \"\\033[91m\")\n",
    "                    + f\"\\t(Difference: {loss_diff:<+10.2e})\"\n",
    "                    + \"\\033[0m\"\n",
    "                    + f\"\\tTime: {time_diff:.2f}, (per epoch: {(time_diff/50):.2f})\"\n",
    "                )\n",
    "\n",
    "    return weights, bias, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "- Determine the sign of the result of $x$ in $w^T x + b$\n",
    "\n",
    "- If the sign is _positive_, the prediction is the transaction is **fraudulent**\n",
    "\n",
    "- If the sign is _negative_, the prediction is the transaction is **not fraudulent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, weights: np.ndarray, bias: float) -> np.ndarray:\n",
    "    \"\"\"Use input weights/bias to predict the label for the input data.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Data used for predictions.\n",
    "        weights (np.ndarray): Weights used to make predictions.\n",
    "        bias (float): Bias used to make predictions.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The predicted labels for the input data.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sign(np.dot(X, weights) + bias).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    prediction_column_name: str,\n",
    ") -> tuple[tuple[float, float, float, float], tuple[float, float, float]]:\n",
    "    TP = ((test[prediction_column_name] == 1) & (test[\"Class\"] == 1)).sum()\n",
    "    TN = ((test[prediction_column_name] == -1) & (test[\"Class\"] == -1)).sum()\n",
    "    FP = ((test[prediction_column_name] == 1) & (test[\"Class\"] == -1)).sum()\n",
    "    FN = ((test[prediction_column_name] == -1) & (test[\"Class\"] == 1)).sum()\n",
    "\n",
    "    precision: float = np.nan\n",
    "    if (TP + FP) > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "\n",
    "    recall: float = np.nan\n",
    "    if (TP + FN) > 0:\n",
    "        recall = TP / (TP + FN)\n",
    "\n",
    "    f1: float = np.nan\n",
    "    if (precision + recall) > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Actual Fraud (Class=1):      {(test['Class'] == 1).sum()}\")\n",
    "    print(f\"Actual Non-Fraud (Class=-1): {(test['Class'] == -1).sum()}\")\n",
    "    print()\n",
    "    print(f\"TP: {TP}\")\n",
    "    print(f\"TN: {TN}\")\n",
    "    print(f\"FP: {FP}\")\n",
    "    print(f\"FN: {FN}\")\n",
    "    print()\n",
    "    print(f\"PRECISION: {precision}\")\n",
    "    print(f\"RECALL:    {recall}\")\n",
    "    print(f\"F1-SCORE:  {f1}\")\n",
    "    print()\n",
    "\n",
    "    return (TP, TN, FP, FN), (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"Class\", axis=1).to_numpy()\n",
    "Y_train = train[\"Class\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "\n",
    "Gaussian (RBF):\n",
    "\n",
    "$$K(x_i, x_j) = \\exp(-\\frac{1}{2 \\sigma^2} ||x_i - x_j||^2) = \\exp(-\\gamma ||x_i - x_j||^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "gamma: float = 0.08\n",
    "n_components: int = 3000\n",
    "\n",
    "rbf_kernel = RBFSampler(\n",
    "    gamma=gamma, n_components=n_components, random_state=42\n",
    ")\n",
    "\n",
    "X_train_kernel = rbf_kernel.fit_transform(X_train)  # Apply the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Fitting\n",
    "C: float = 1.0\n",
    "epochs: int = 2000\n",
    "learning_rate: float = 0.05\n",
    "class_weights: dict[int, float] = {-1: 1.0, 1: 50.0}\n",
    "decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0     Loss = 1.11e+06  \u001b[91m\t(Difference: +0.00e+00 )\u001b[0m\tTime: 0.62, (per epoch: 0.01)\n",
      "Epoch 50    Loss = 5.40e+05  \u001b[92m\t(Difference: -5.66e+05 )\u001b[0m\tTime: 30.87, (per epoch: 0.62)\n",
      "Epoch 100   Loss = 1.79e+04  \u001b[92m\t(Difference: -5.22e+05 )\u001b[0m\tTime: 30.69, (per epoch: 0.61)\n",
      "Epoch 150   Loss = 7.96e+02  \u001b[92m\t(Difference: -1.71e+04 )\u001b[0m\tTime: 30.58, (per epoch: 0.61)\n",
      "Epoch 200   Loss = 7.90e+02  \u001b[92m\t(Difference: -6.38e+00 )\u001b[0m\tTime: 30.84, (per epoch: 0.62)\n",
      "Epoch 250   Loss = 7.90e+02  \u001b[91m\t(Difference: +7.32e-01 )\u001b[0m\tTime: 30.49, (per epoch: 0.61)\n",
      "Epoch 300   Loss = 7.92e+02  \u001b[91m\t(Difference: +2.09e+00 )\u001b[0m\tTime: 688.80, (per epoch: 13.78)\n",
      "Epoch 350   Loss = 8.05e+02  \u001b[91m\t(Difference: +1.27e+01 )\u001b[0m\tTime: 33.68, (per epoch: 0.67)\n",
      "Epoch 400   Loss = 8.00e+02  \u001b[92m\t(Difference: -4.99e+00 )\u001b[0m\tTime: 33.68, (per epoch: 0.67)\n",
      "Epoch 450   Loss = 7.94e+02  \u001b[92m\t(Difference: -6.26e+00 )\u001b[0m\tTime: 35.47, (per epoch: 0.71)\n",
      "Epoch 500   Loss = 9.05e+02  \u001b[91m\t(Difference: +1.11e+02 )\u001b[0m\tTime: 36.01, (per epoch: 0.72)\n",
      "Epoch 550   Loss = 7.98e+02  \u001b[92m\t(Difference: -1.07e+02 )\u001b[0m\tTime: 35.90, (per epoch: 0.72)\n",
      "Epoch 600   Loss = 7.92e+02  \u001b[92m\t(Difference: -6.15e+00 )\u001b[0m\tTime: 36.81, (per epoch: 0.74)\n",
      "Epoch 650   Loss = 1.29e+03  \u001b[91m\t(Difference: +4.94e+02 )\u001b[0m\tTime: 33.45, (per epoch: 0.67)\n",
      "Epoch 700   Loss = 7.99e+02  \u001b[92m\t(Difference: -4.86e+02 )\u001b[0m\tTime: 32.40, (per epoch: 0.65)\n",
      "Epoch 750   Loss = 7.93e+02  \u001b[92m\t(Difference: -6.07e+00 )\u001b[0m\tTime: 32.18, (per epoch: 0.64)\n",
      "Epoch 800   Loss = 7.91e+02  \u001b[92m\t(Difference: -1.96e+00 )\u001b[0m\tTime: 31.31, (per epoch: 0.63)\n",
      "Epoch 850   Loss = 8.04e+02  \u001b[91m\t(Difference: +1.21e+01 )\u001b[0m\tTime: 30.89, (per epoch: 0.62)\n",
      "Epoch 900   Loss = 7.92e+02  \u001b[92m\t(Difference: -1.18e+01 )\u001b[0m\tTime: 30.98, (per epoch: 0.62)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit the Model based on the previously specified parameters.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m w, b, losses = \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWEIGHTS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(X, Y, learning_rate, C, epochs, class_weights, decay, log)\u001b[39m\n\u001b[32m     54\u001b[39m losses: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28mlist\u001b[39m()\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Determine SVM Margins\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     margins: np.ndarray = \u001b[32m1\u001b[39m - Y * (\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m + bias)\n\u001b[32m     59\u001b[39m     C_weights: np.ndarray = np.where(\n\u001b[32m     60\u001b[39m         Y == \u001b[32m1\u001b[39m, C * class_weights[\u001b[32m1\u001b[39m], C * class_weights[-\u001b[32m1\u001b[39m]\n\u001b[32m     61\u001b[39m     )\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# Determine Gradients - updated using points that are in the margin or misclassified.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fit the Model based on the previously specified parameters.\n",
    "w, b, losses = fit(\n",
    "    X_train_kernel,\n",
    "    Y_train,\n",
    "    learning_rate=learning_rate,\n",
    "    C=C,\n",
    "    epochs=epochs,\n",
    "    class_weights=class_weights,\n",
    "    decay=decay,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f\"WEIGHTS: {w}\")\n",
    "print(f\"BIAS:    {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(losses)) * 50, losses)\n",
    "\n",
    "fig.suptitle(\"Soft-SVM for Credit Card Detection\", fontsize=16)\n",
    "fig.text(\n",
    "    0.5,\n",
    "    0.9,\n",
    "    f\"C={C}, learning rate={learning_rate}, learning rate decay={decay}\",\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "ax.set_title(\"Loss Over Time\")\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    col for col in test.columns if col not in [\"Class\", \"PredictedClass\"]\n",
    "][:30]\n",
    "X_test = test[feature_columns].to_numpy()\n",
    "Y_test = test[\"Class\"].to_numpy()\n",
    "\n",
    "X_test_kernel = rbf_kernel.transform(X_test)\n",
    "\n",
    "test[\"PredictedClass\"] = predict(X_test_kernel, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.dot(X_test_kernel, w) + b\n",
    "\n",
    "plt.hist(scores[Y_test == 1], bins=50, alpha=0.5, label=\"fraud\")\n",
    "plt.hist(scores[Y_test == -1], bins=50, alpha=0.5, label=\"normal\")\n",
    "\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"-\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Decision Scores by Class\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP, TN, FP, FN), (precision, recall, f1) = evaluate(\n",
    "    prediction_column_name=\"PredictedClass\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()  # Show what the dataframe looks like with the predicted class column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = [\n",
    "#     col for col in test.columns if col not in [\"Class\", \"PredictedClass\"]\n",
    "# ][:30]\n",
    "\n",
    "# epochs = 5000\n",
    "# for C in (0.1, 1, 10, 100):\n",
    "#     for w1 in (10, 50, 100, 500):\n",
    "#         class_weights: dict[int, float] = {-1: 1.0, 1: w1}\n",
    "#         for learning_rate in (0.001, 0.005):\n",
    "#             print(\n",
    "#                 f\"PARAMETERS: C: {C}, w1: {w1}, learning rate: {learning_rate}\\n\"\n",
    "#             )\n",
    "\n",
    "#             w, b, losses = fit(\n",
    "#                 X_train,\n",
    "#                 Y_train,\n",
    "#                 learning_rate=learning_rate,\n",
    "#                 C=C,\n",
    "#                 epochs=epochs,\n",
    "#                 class_weights=class_weights,\n",
    "#                 decay=1e-3,\n",
    "#                 log=False,\n",
    "#             )\n",
    "\n",
    "#             print(f\"WEIGHTS: {w}\")\n",
    "#             print(f\"BIAS:    {b}\\n\")\n",
    "\n",
    "#             X_test = test[feature_columns].to_numpy()\n",
    "#             Y_test = test[\"Class\"].to_numpy()\n",
    "\n",
    "#             test[\"PredictedClass\"] = predict(X_test, w, b)\n",
    "#             evaluate(prediction_column_name=\"PredictedClass\")\n",
    "\n",
    "#             print(\"~\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Using Existing SciKit Learn Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# sklearn_SVC = SVC(\n",
    "#     kernel=\"linear\",\n",
    "#     C=C,\n",
    "#     random_state=42,\n",
    "#     class_weight=class_weights,\n",
    "#     max_iter=epochs,\n",
    "# )\n",
    "# sklearn_SVC.fit(X_train, Y_train)\n",
    "\n",
    "# y_pred_sklSVC = sklearn_SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"SKLearnSVCPredictedClass\"] = y_pred_sklSVC\n",
    "# evaluate(prediction_column_name=\"SKLearnSVCPredictedClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import OneClassSVM\n",
    "\n",
    "# sklearn_1CSVM = OneClassSVM(\n",
    "#     kernel=\"rbf\", nu=0.01, gamma=\"scale\", max_iter=epochs\n",
    "# )\n",
    "# sklearn_1CSVM.fit(X_train[Y_train == -1])\n",
    "\n",
    "# y_scores = sklearn_1CSVM.decision_function(X_test)\n",
    "# y_pred_skl1CSVM_raw = sklearn_1CSVM.predict(X_test)\n",
    "\n",
    "# y_pred_skl1CSVM = np.where(y_pred_skl1CSVM_raw == -1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"SKLearn1CSVMPredictedClass\"] = y_pred_skl1CSVM\n",
    "# evaluate(prediction_column_name=\"SKLearn1CSVMPredictedClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS5402",
   "language": "python",
   "name": "cs5402"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
